





#! -*- coding:utf-8 -*-
import datetime
import re
import time

import requests
# import pymongo
import pymysql
from multiprocessing import Pool
from selenium import webdriver

# 捕获异常
from lxml import etree





def get_one_page(url):



    driver.get(url)
    html = driver.page_source
    return html

def parse_html(html):
    big_list = []
    f_list = []

    selector = etree.HTML(html)
    all_profits = selector.xpath('//*[@id="ProfitStatementNewTable0"]/tbody/tr[23]/td/text()')

    f_profile =[ "".join(x.split(",")) for x in all_profits]

    name = selector.xpath('//*[@id="toolbar"]/div[1]/h1/a/text()')
    code = selector.xpath('//*[@id="toolbar"]/div[1]/h2/text()')
    first_info = name + code + f_profile


    l_t = tuple(first_info)
    f_list.append(l_t)
    return f_list




def insertDB(content):
    connection = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='123456', db='a_stock',
                                 charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)
    cursor = connection.cursor()
    cursor.executemany("insert into code_netProfits (name,code,v1,v2,v3,v4,v5) values (%s,%s,%s,%s,%s,%s,%s)", content)
    connection.commit()
    connection.close()
    print('向MySQL中添加数据成功！')


# 平静还是在于像mysql中批量插入的问题

# 下载遍历的url列表

# 把　A股爬虫数据进行两方面优化：

# 1. 财务数据只取近５个季度的
# 2.同时加入板块的索引一部插入到新表中
if __name__ == '__main__':
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome("/usr/bin/chromedriver", chrome_options=options)
    hs300_code = ["000415", "601216", "601877", "002241", "600489", "000963", "600867", "601360", "002958", "600221",
                  "002460", "000423", "600733", "002050", "002027", "601899", "601162", "002032", "603160", "600297",
                  "000100", "600522", "600547", "603260", "601212", "000656", "002142", "601009", "002594", "600027",
                  "000413", "000725", "600036", "600498", "000001", "603799", "600928", "002466", "601336", "002493",
                  "000895", "600023", "000627", "000069", "601766", "002311", "600519", "600000", "601166", "601138",
                  "600487", "601939", "601601", "601988", "601225", "600011", "600926", "000166", "600233", "600016",
                  "601169", "601998", "600299", "000333", "600188", "002024", "600061", "601628", "600848", "603993",
                  "601818", "601577", "601328", "601288", "600977", "600919", "600816", "600703", "600016", "300024",
                  "000703", "000166", "601066", "600900", "000776", "601997", "002271", "600004", "601318", "600015",
                  "600837", "601788", "002736", "601398", "601881", "601108", "600276", "300408", "600018", "601211",
                  "601211", "300017", "600705", "601898", "000063", "300144", "000858", "600177", "000723", "600837",
                  "603288", "002939", "002010", "002294", "601229", "000728", "600655", "601838", "601618", "002508",
                  "600570", "000898", "601555", "600535", "600019", "600176", "002945", "001979", "600028", "601985",
                  "002739", "601377", "600705", "600795", "600390", "600030", "601901", "600309", "601155", "600352",
                  "601668", "601688", "600019", "600085", "300498", "600958", "600030", "600208", "600398", "600369",
                  "601857", "601828", "601618", "600809", "600029", "600068", "600089", "600104", "600999", "601021",
                  "601669", "601727", "600968", "601021", "600271", "601006", "601018", "002001", "600362", "600153",
                  "600170", "600115", "600999", "000709", "002081", "600637", "601800", "600109", "600332", "000338",
                  "601088", "000630", "600153", "601186", "002411", "600109", "601633", "002673", "002841", "600690",
                  "601800", "600741", "600887", "002230", "600438", "601111", "601989", "603259", "601878", "601992",
                  "600406", "600438", "002916", "600674", "002415", "002422", "002456", "002624", "002236", "002153",
                  "002558", "601298", "601390", "000538", "600111", "300003", "000157", "300070", "002007", "600482",
                  "601198", "002044", "600663", "300124", "000651", "000568", "002146", "300070", "000630", "000157",
                  "002202", "300033", "600606", "300059", "002601", "601012", "600585", "600340", "002475", "600009",
                  "002044", "600009", "002410", "300413", "601238", "600010", "300433", "600340", "000671", "300015",
                  "300347", "601808", "000002", "000629", "002252", "000425", "601888", "002773", "600998", "002352",
                  "000661", "300136", "002304", "600346", "600989", "002602", "603986", "603833", "600893", "600100",
                  "002607", "000938", "002008", "002179", "600583", "000625", "600372", "603899", "000876", "600031",
                  "600031", "600436", "600516", "600066", "002120", "000768", "002714", "300142", "600038", "600183",
                  "002555", "603019", "002938", "601698", "601933", "600118", "300122", "600760", "600588", "603501"]



    for num_code in hs300_code:
        url = 'http://vip.stock.finance.sina.com.cn/corp/go.php/vFD_ProfitStatement/stockid/{0}/ctrl/part/displaytype/4.phtml'.format(num_code)



        html = get_one_page(url)
        try:
            time.sleep(3)
            co = parse_html(html)
            print(url)
            print(co)
            insertDB(co)
        except :
            pass





# 筛选一季度最赚钱的贵金属企业
# select * from table_Finances WHERE type="贵金属" order by v1 desc;
# v4---->v1  就近

#
# create table code_netProfits(
# id int not null primary key auto_increment,
# name varchar(11),
# code varchar(11),
# v1 text,
# v2 text,
# v3 text,
# v4 text,
# v5 text
# ) engine=InnoDB default charset=utf8;


# drop table code_netProfits;


# 1. 建表，先用id+TIME 把表结构建起来，用平安银行的日期把时间列给撑起来
# 2. sql语句每次，爬取一次就，先插入一列，列名为s+代码，然后再插入数据！批量爬取，批量进行操作！



